df= read.csv('~/Documents/GitHub/18FMath564-pj/data/train_processed.csv')
tdf= read.csv('~/Documents/GitHub/18FMath564-pj/data/test_processed.csv')
#remove first two columns which are index
df=df[,-c(1,2)]
tdf=tdf[,-c(1,2)]
# scale data since scaled data are required in some models
tdf_scale = scale(tdf,center=FALSE)
numv = names(train_procdata[,sapply(train_procdata,is.numeric)]) # numeric
numv
numv = names(train_procdata[,sapply(tdf,is.numeric)]) # numeric
numv = names(tdf[,sapply(tdf,is.numeric)]) # numeric
numv
# scale data since scaled data are required in some models
tdf_scale = scale(tdf[,numv],center=FALSE)
tdf_scale = data.frame(tdf_scale,tdf[,-numv])
tdf[,numv]
tdf[,-numv]
tdf[,c(-numv)]
catv = names(tdf[,sapply(tdf,is.factor)]) # numeric
catv
logv = names(tdf[,sapply(tdf,is.logical)]) # binary
logv
tdf_scale = data.frame(tdf_scale,tdf[,catv],tdf[,logv])
set.seed(564)
lm = train(SalePrice~., data = tdfmethod='lm',trControl=controlParameter)
summary(lm)
set.seed(564)
lm = train(SalePrice~., data = tdf,method='lm',trControl=controlParameter)
summary(lm)
# perform cross-validtion on training data set
library(caret)
install.packages('caret')
# perform cross-validtion on training data set
library(caret)
controlParameter=trainControl(method = "cv",number = 10,savePredictions = TRUE)
set.seed(564)
lm = train(SalePrice~., data = tdf,method='lm',trControl=controlParameter)
summary(lm)
df= read.csv('~/Documents/GitHub/18FMath564-pj/data/train_processed.csv')
testdf= read.csv('~/Documents/GitHub/18FMath564-pj/data/test_processed.csv')
#remove first two columns which are index
df=df[,-c(1,2)]
testdf=testdf[,-c(1,2)]
set.seed(564)
lm = train(SalePrice~., data = df,method='lm',trControl=controlParameter)
summary(lm)
library(leap)
?caret::train()
set.seed(564)
lm_forward=train(SalePrice~.,data=df,method='leapForward',trControl=controlParameter)
# variable selection by AIC stepwise algorithm
library(leaps)
# tree regression via rpart package
# grow the tree
tr=rpart(df$SalePrice~.,data = df, method = 'anova')
set.seed(564)
lm_forward=train(SalePrice~.,data=df,method='leapForward',trControl=controlParameter)
set.seed(564)
lm_step=train(SalePrice~.,data=df,method='leapSeq',trControl=controlParameter)
warnings()
lm_stepAIC=train(SalePrice~.,data=df,method='glmStepAIC',trControl=controlParameter)
# Make Prediction
lm_pred = predict(lm,df)
# To calculate AdjR2
lm_adjR2 = adjR2_test(lm_pred,df$SalePrice)
# define Adjusted R square function
adjR2_test = function (pred,y.test){
r2=miscTools::rSquared(y.test,resid = y.test-pred)
n=nrow(x.test)
q=ncol(x.test)
adjr2=1-((1-r2^2)*(n-1)/(n-q))
}
# define rmlse
rmlse <- function(yhat, y) {
n <- length(yhat)
return(sqrt((1/n)*sum((log(yhat)-log(y))^2)))
}
# To calculate AdjR2
lm_adjR2 = adjR2_test(lm_pred,df$SalePrice)
# To calculate rmlse
lm_rmlse = rmlse(abs(lm_pred), df$SalePrice)
#When we include the intercept it give ceofficient for pop.density=NA
#using the modelform include intercept=FALSE does not change the pop.density=NA
#The only solution that works was to manually type out the equation including -1
#This resulted in pop.denisty !=NA.
modelForm
?rs2
?rsquare
??rsquare
summary(lm)
df= read.csv('~/Documents/GitHub/18FMath564-pj/data/train_processed.csv')
testdf= read.csv('~/Documents/GitHub/18FMath564-pj/data/test_processed.csv')
#remove first two columns which are index
df=df[,-c(1,2)]
testdf=testdf[,-c(1,2)]
# define rmlse function
rmlse <- function(yhat, y) {
n <- length(yhat)
return(sqrt((1/n)*sum((log(yhat)-log(y))^2)))
}
# perform cross-validtion on training data set
library(caret)
controlParameter=trainControl(method = "cv",number = 10,savePredictions = TRUE)
set.seed(564)
lm = train(SalePrice~., data = df,method='lm',trControl=controlParameter)
coef(lm)
summary(lm)
# Feature selection by AIC stepwise algorithm
# Errors for below three method, I am not sure why is that
library(leaps)
lm_forward=train(SalePrice~.,data=df,method='leapForward',trControl=controlParameter)
warnings()
# Tree-Based model
# CART Tree
tree_cp=train(SalePrice~.,data=df,method='rpart',trControl=controlParameter)
tree_cp
?cv.glmnet()
# models with penalty
# Ridge regression
lambdas <- 10^seq(10, -2, length = 100)
ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
glmnet::cv.glmnet()
# models with penalty
# Ridge regression
lambdas = 10^seq(10, -2, length = 100)
# ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
ridgeGrid=expand.grid(alpha=0,lambda=lambdas)
lm_ridge=train(SalePrice~., data=df, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
summary(lm_ridge)
coef(lm_ridge)
# Elasticnet regression
lm_elas=train(SalePrice~., data=df, method = 'glmnet', trControl=controlParameter)
summary(lm_elas)
coef(lm_elas)
ridgeGrid=expand.grid(alpha=0,lambda=c(0,5,0.0001))
lm_ridge=train(SalePrice~., data=df, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
coef(lm_ridge)
rmse <- function(yhat, y) {
n <- length(yhat)
return(sqrt((1/n)*sum((yhat-y)^2)))
}
numv = names(df[,sapply(df,is.numeric)]) # numeric
numv
catv = names(df[,sapply(df,is.factor)]) # numeric
catv
logv = names(df[,sapply(df,is.logical)]) # binary
logv
df_scale = scale(df[,numv],center=FALSE)
df_scale = data.frame(tdf_scale,df[,catv],df[,logv])
df_scale = data.frame(df_scale,df[,catv],df[,logv])
ridgeGrid=expand.grid(alpha=0,lambda=c(0,5,0.0001))
lm_ridge=train(SalePrice~., data=df_scale, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
lm_ridge
summary(lm_ridge)
coef(lm_ridge)
coefficients(lm_ridge)
lm_ridge
# ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
ridgeGrid=expand.grid(alpha=0,lambda=lambdas)
lm_ridge=train(SalePrice~., data=df_scale, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
lm_ridge
df= read.csv('~/Documents/GitHub/18FMath564-pj/data/train_processed.csv')
testdf= read.csv('~/Documents/GitHub/18FMath564-pj/data/test_processed.csv')
#remove first two columns which are index
df=df[,-c(1,2)]
testdf=testdf[,-c(1,2)]
# define rmlse function
rmse <- function(yhat, y) {
n <- length(yhat)
return(sqrt((1/n)*sum((yhat-y)^2)))
}
library(caret)
controlParameter=trainControl(method = "cv",number = 10,savePredictions = TRUE)
# linear regression
set.seed(564)
lambdas = 10^seq(10, -2, length = 100)
# ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
ridgeGrid=expand.grid(alpha=0,lambda=lambdas)
lm_ridge=train(SalePrice~., data=df_scale, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
# Ridge regression
lambdas = 10^seq(10, -2, length = 100)
# ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
ridgeGrid=expand.grid(alpha=0,lambda=lambdas)
lm_ridge=train(SalePrice~., data=df, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
length(lm_ridge)
names(lm_ridge)
str(lm_ridge)
controlParameter=trainControl(method = "cv",number = 2,savePredictions = TRUE)
lambdas = 10^seq(10, -2, length = 100)
# ridge_fit <- cv.glmnet(train_matrix, y, alpha=0, lambda=lambdas)
ridgeGrid=expand.grid(alpha=0,lambda=lambdas)
lm_ridge=train(SalePrice~., data=df, method = 'glmnet',trControl=controlParameter,tuneGrid=ridgeGrid)
length(lm_ridge)
lm_ridge_pred=predict(lm_ridge,df)
lm_ridge_rmse=rmse(abs(lm_ridge), df$SalePrice)
lm_ridge_rmse=rmse(abs(lm_ridge_pred), df$SalePrice)
lassoGrid=expand.grid(alpha=1,lambda=lambdas)
lm_lasso=train(SalePrice~., data=df, method = 'glmnet',trControl=controlParameter,tuneGrid=lassoGrid)
# Elasticnet regression
lm_elas=train(SalePrice~., data=df, method = 'glmnet', trControl=controlParameter)
lm_ridge_pred=predict(lm_ridge,df)
lm_elas_pred=predict(lm_elas,df)
lm_ridge_rmse=rmse(abs(lm_ridge_pred), df$SalePrice)
lm_elas_rmse=rmse(abs(lm_elas_pred), df$SalePrice)
lm_lasso_rmse=rmse(abs(lm_lasso_pred), df$SalePrice)
lm_lasso_pred=predict(lm_lasso,df)
lm_lasso_rmse=rmse(abs(lm_lasso_pred), df$SalePrice)
# Tree-Based model
# CART Tree, cannot set method to be anova
tree_cp=train(SalePrice~.,data=df,method='rpart',trControl=controlParameter)
tree_cp_pred=predict(tree_cp,df)
tree_cp_rmse=rmse(abs(tree_cp_pred), df$SalePrice)
lm = train(SalePrice~., data = df,method='lm',trControl=controlParameter)
# Make Prediction
lm_pred = predict(lm,df)
# To calculate rmse
lm_rmse = rmse(abs(lm_pred), df$SalePrice)
lm_forward=train(SalePrice~.,data=df,method='leapForward',trControl=controlParameter)
summary(lm_forward)
lm_forward$call
lm_forward$coefnames
lm_forward$finalModel
rmlse <- function(yhat, y) {
n <- length(yhat)
return(sqrt((1/n)*sum((log(yhat)-log(y))^2)))
}
# To calculate rmse
lm_rmse = rmlse(abs(lm_pred), df$SalePrice)
# lm_forward_rmse = rmse(abs(lm_forward_pred), df$SalePrice)
# lm_backward_rmse = rmse(abs(lm_backward_pred), df$SalePrice)
# lm_step_rmse = rmse(abs(lm_step_pred), df$SalePrice)
lm_stepAIC_rmse = rmlse(abs(lm_stepAIC_pred), df$SalePrice)
lm_lasso_rmse=rmlse(abs(lm_lasso_pred), df$SalePrice)
lm_ridge_rmse=rmlse(abs(lm_ridge_pred), df$SalePrice)
lm_elas_rmse=rmlse(abs(lm_elas_pred), df$SalePrice)
tree_cp_rmse=rmlse(abs(tree_cp_pred), df$SalePrice)
lm_ridge$bestTune
min(lm_ridge$results$RMSE)
